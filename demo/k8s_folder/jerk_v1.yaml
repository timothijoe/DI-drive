 apiVersion: diengine.opendilab.org/v2alpha1
 kind: DIJob
 metadata:
   name: jerk_try_v1
 spec:
   group: "base"  # 标记job所属的group
   priority: "normal"  # 表示job的优先级，保留字段，调度中或许可以用到
   cleanPodPolicy: "None"  # 表示job运行完成之后，如何处理worker pods
   preemptible: false  # 表示job是否允许被抢占，调度中对job资源改动之后涉及到抢占操作
   minReplicas: 1
   maxReplicas: 1
   engineFields:
     topology: "star"  # 表示workers之间的拓扑连接
     parallelWorkers: 1  # 表示每个worker中启动的parallel workers数目
   template:
     spec:
       containers:
       - name: di-container
         image: opendilab/ding:nightly
         imagePullPolicy: Always  # IfNotPresent
         env:
         - name: PYTHONUNBUFFERED
           value: "1" 
         resources:
           requests:
             nvidia.com/gpu: 1
             cpu: 16
             memory: "25Gi"
           limits:
             nvidia.com/gpu: 1
             cpu: 20
             memory: "50Gi"
         command: ["/bin/bash", "-c",]
         args:
         - |
           cd /mnt/nfs/zhoutong/latern/DI-engine
           pip install -e .
           cd /mnt/nfs/zhoutong/latern/metadrive
           pip install -e .
           cd /mnt/nfs/zhoutong/latern/xad
           pip install -e .
           python3 -u /mnt/nfs/zhoutong/latern/xad/metadrive/train_k8s/jerk_v1.py
         volumeMounts:
         - name: work-dir
           mountPath: /mnt/nfs/niuyazhe
       volumes:
       - name: cache-volume
         emptyDir:
           medium: Memory
           sizeLimit: 128Mi
       - name: work-dir
         hostPath:
           path: /mnt/nfs/niuyazhe